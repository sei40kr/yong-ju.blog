---
title: "確率微分方程式"
date: 2022-06-04T19:09:52+09:00
categories: ["数学"]
tags: ["数学", "大学数学", "確率統計", "確率微分方程式", "WIP"]
---

import { Link } from "@builder.io/qwik-city";
import { Details } from "~/components/post/details";
import { Hint } from "~/components/post/hint";

## 前提知識

- <Link href="/posts/statistics">確率統計</Link>

## ランダムウォーク

点 $P$ は時刻 $t=0$ で原点にあり、1秒ごとに等しい確率で $+\sigma$ または $-\sigma$ だけ移動するものとする。
$t$ 秒後の点 $P$ の位置を $R(t)$ とすると、$R(t)$ を **ランダムウォーク (random walk)** という。

$R(t)$ は次のような確率変数となる。

$$
\begin{aligned}
E[R(t)] &= 0 \\
V[R(t)] &= \sigma^2 t
\end{aligned}
$$

<Details title="E[R(t)], V[R(t)] の導出">

$$
\Delta R(t) = R(t + \Delta t) - R(t)
$$

とおくと、$\Delta R(t)$ は次のような確率変数となる。

$$
\begin{aligned}
E[\Delta R(t)] &= 0 \\
V[\Delta R(t)] &= \sigma^2
\end{aligned}
$$

よって

$$
\begin{aligned}
E[R(t)] &= E[\Delta R(0) + \Delta R(1) + \cdots + \Delta R(t)] \\
  &= E[\Delta R(0)] + E[\Delta R(1)] + \cdots + E[\Delta R(t)] \\
  &= 0 \\
V[R(t)] &= V[\Delta R(0) + \Delta R(1) + \cdots + \Delta R(t)] \\
  &= V[\Delta R(0)] + V[\Delta R(1)] + \cdots + V[\Delta R(t)] \\
  &= \sigma^2 + \sigma^2 + \cdots + \sigma^2 \\
  &= \sigma^2 t
\end{aligned}
$$

</Details>

## ウィーナー過程

点 $P$ は時刻 $t=0$ で原点にあり、$\Delta t = \frac{1}{n}$ ごとに等しい確率で $+\Delta W$ または $-\Delta W$ だけ移動するものとする。
このとき $t$ 秒後の点 $P$ の位置を $W(t)$ とする。

移動を $k$ 回行った後の点 $P$ の位置 $W(\frac{k}{n})$ は次のような確率変数となる。

$$
\begin{aligned}
E[W(\frac{k}{n})] &= 0 \\
V[W(\frac{k}{n})] &= \Delta W^2 k
\end{aligned}
$$

$\Delta W = \pm \sigma \sqrt{\frac{1}{n}} = \pm \sigma \sqrt{\Delta t}$ としたときの $\lim_{n \to \infty} W(t)$ を **ウィーナー過程 (Wiener process)** という。

<Hint level="info">
  **ウィーナー過程はランダムウォークの連続極限である**
  
$n = 1, \Delta W = \pm \sigma \sqrt{\Delta t}$ としたときの $W(t)$ はランダムウォークとなる。
  
$$
V[W(t)] = \sigma^2 t = V[R(t)]
$$

</Hint>

<Hint level="info">
  **ウィーナー過程の別の定義**

$W(t)$ が次の条件を満たすとき、$W(t)$ をウィーナー過程という。

$$
\begin{aligned}
W(0) &= 0 \\
W(t_0 + t) - W(t_0) &\sim N(0, \sigma^2 t)
\end{aligned}
$$

</Hint>

<Hint level="info">
  **ウィーナー過程の性質**
  
$W(t)$ の速度は $\frac{dW}{dt} = \pm \sigma \frac{1}{\sqrt{t}} = \pm \infty$ であるにも関わらず、$t = 0$ における点 $P$ の位置 $W(1)$ は

$$
\begin{aligned}
W(1) &= \lim_{n \to \infty} \sum_{k=1}^{n} \Delta W
\end{aligned}
$$

となり、$N(0, \sigma^2)$ に従う有限の値となる。

$V[W(1)]$ も同様に

$$
\begin{aligned}
V[W(1)] &= \lim_{n \to \infty} \sum_{k=1}^{n} V[\Delta W] \\
  &= \lim_{n \to \infty} \sum_{k=1}^{n} \sigma^2 \Delta t \\
  &= \sigma^2
\end{aligned}
$$

となり、有限の値となる。

</Hint>

## 確率微分方程式

$X(t)$ が次の確率微分方程式を満たすとき、$X(t)$ を **確率微分方程式 (stochastic differential equation)** という。

$$
dX(t) = \mu(X(t), t) dt + \sigma(X(t), t) dW(t)
$$

この解は (存在すれば) 次のように表される。

$$
X(t) = X(0) + \int_{0}^{t} \mu(X(s), s) ds + \int_{0}^{t} \sigma(X(s), s) dW(s)
$$

## 伊藤積分

確率微分方程式 $dX(t) = \mu(X(t), t) dt + \sigma(X(t), t) dW(t)$ の解において、右辺の第2項の積分 $\int_{0}^{t} \sigma(X(s), s) dW(s)$ は普通の積分とは異なる。
この積分を次のように定義したものを **伊藤積分 (Itô integral)** という。

$$
\int_{0}^{t} \sigma(X(s), s) dW(s) = \sum_{k=1}^{n} \sigma(X(t_{k-1}), t_{k-1}) [W(t_k) - W(t_{k-1})]
$$

<Details title="伊藤積分を定義する理由">
確率微分方程式 $dX = bW(t) dW$ の解を考える。

普通に両辺を積分すると

$$
\begin{aligned}
X(t) &= X(0) + \int_{0}^{t} b W(s) dW \\
  &= X(0) + \dfrac{b}{2} W^2(t)
\end{aligned}
$$

となりそうだが、$b = 1, X(0) = 0, X(5) = 10, W(5) = 10$ とすると

$$
\begin{aligned}
\Delta X(5) &= X(6) - X(5) \\
  &\approx b W(5) [W(6) - W(5)] \\
  &= 10 [W(6) - W(5)]
\end{aligned}
$$

ここで $W(6) - W(5) = \Delta W(5) \sim N(0, 1)$ なので、$0.17$ 程度の確率で $\Delta W(5) < -1, \Delta X(5) < -10$ となり、$X(6) < 0$ となる。
しかし、普通の積分の解は $X(6) = \frac{1}{2} W^2(6) > 0$ となるため、矛盾する。

伊藤積分の定義で $bW(t) dW$ を積分すると

$$
\begin{aligned}
X(t) &= X(0) + \sum_{k=1}^{n} b W(t_{k-1}) [W(t_k) - W(t_{k-1})] \\
  &= X(0) + \sum_{k=1}^{n} b \left\{ \dfrac{1}{2} [W(t_k) + W(t_{k-1})] - \dfrac{1}{2} [W(t_k) - W(t_{k-1})] \right\} [W(t_k) - W(t_{k-1})] \\
  &= X(0) + \dfrac{b}{2} \sum_{k=1}^{n} [W^2(t_k) - W^2(t_{k-1})] - \dfrac{b}{2} \sum_{k=1}^{n} [W(t_k) - W(t_{k-1})]^2
\end{aligned}
$$

ここで

$$
\begin{aligned}
\sum_{k=1}^{n} [W^2(t_k) - W^2(t_{k-1})] &= W^2(t_n) - W^2(t_0) \\
  &= W^2(t) - W^2(0) \\
  &= W^2(t) \\
\sum_{k=1}^{n} [W(t_k) - W(t_{k-1})]^2 &= \sum_{k=1}^{n} \Delta W^2(t_{k-1}) \\
  &= \sum_{k=1}^{n} \sigma^2 (t_k - t_{k-1}) \\
  &= \sigma^2 t
\end{aligned}
$$

これを代入すると

$$
X(t) = X(0) + \dfrac{b}{2} W^2(t) - \dfrac{b}{2} \sigma^2 t
$$

となるため、矛盾が生じない。

</Details>
